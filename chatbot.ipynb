{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eed2d1-5bf6-4f1f-bbdb-75b9ee3cd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Downloads already confirmed as done\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Simple predefined responses\n",
    "responses = {\n",
    "    \"hello\": [\"Hi there!\", \"Hello!\", \"Hey!\"],\n",
    "    \"how are you\": [\"I'm good, thanks!\", \"Doing well. You?\", \"All good here!\"],\n",
    "    \"bye\": [\"Goodbye!\", \"See you later!\", \"Take care!\"],\n",
    "    \"what is your name\": [\"I'm a chatbot!\", \"Call me Chatty!\", \"I'm your assistant.\"],\n",
    "    \"thank you\": [\"You're welcome!\", \"No problem!\", \"Glad to help!\"]\n",
    "}\n",
    "\n",
    "# Preprocess input\n",
    "def preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Generate chatbot response\n",
    "def get_response(user_input):\n",
    "    processed = preprocess(user_input)\n",
    "    for key in responses:\n",
    "        if key in processed:\n",
    "            return random.choice(responses[key])\n",
    "    return \"I'm not sure how to respond to that.\"\n",
    "\n",
    "# UI using ipywidgets\n",
    "input_box = widgets.Text(\n",
    "    placeholder='Type your message here...',\n",
    "    description='You:',\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_enter(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        user_input = change['new']\n",
    "        input_box.value = ''  # clear the input box\n",
    "        with output_area:\n",
    "            print(\"You:\", user_input)\n",
    "            print(\"Chatbot:\", get_response(user_input))\n",
    "            print()\n",
    "\n",
    "input_box.observe(on_enter)\n",
    "\n",
    "# Display the chatbot interface\n",
    "display(widgets.VBox([input_box, output_area]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7bb6b-7a11-4e7b-8634-bb30f4c22ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203fa44-ee0f-4895-8b66-ca25ecffb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c271f50-0a67-454a-8a5b-237ccb321ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Predefined Q&A pairs\n",
    "qa_pairs = {\n",
    "    \"hello\": \"Hi there! How can I help you today?\",\n",
    "    \"how are you\": \"I'm just a bot, but I'm doing great. How can I assist you?\",\n",
    "    \"what is your name\": \"I'm a simple AI chatbot using NLTK!\",\n",
    "    \"bye\": \"Goodbye! Have a nice day!\",\n",
    "    \"who created you\": \"I was created using Python and NLTK library.\",\n",
    "    \"what is nlp\": \"Natural Language Processing (NLP) is a field of AI focused on understanding and processing human languages.\"\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess and clean input\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Function to generate response\n",
    "def generate_response(user_input):\n",
    "    user_input = preprocess(user_input)\n",
    "    for question in qa_pairs:\n",
    "        if question in user_input:\n",
    "            return qa_pairs[question]\n",
    "    return \"Sorry, I didn't understand that. Can you try asking something else?\"\n",
    "\n",
    "# Chat Loop\n",
    "print(\"ðŸ¤– ChatBot is online. Type 'bye' to exit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if 'bye' in user_input.lower():\n",
    "        print(\"ChatBot: Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "    response = generate_response(user_input)\n",
    "    print(\"ChatBot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f372adea-6f62-4157-b09a-837a9dbd506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot ðŸ¤– is ready! Type 'bye' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hii\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dell/nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatBot: Goodbye! ðŸ‘‹\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatBot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generate_response(user_input))\n",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(user_input):\n\u001b[1;32m---> 31\u001b[0m     processed_input \u001b[38;5;241m=\u001b[39m preprocess(user_input)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m qa_pairs:\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m processed_input:\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(sentence):\n\u001b[0;32m     24\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, string\u001b[38;5;241m.\u001b[39mpunctuation))\n\u001b[1;32m---> 25\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n\u001b[0;32m     26\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dell/nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download necessary data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 2: Import packages\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Step 3: Prepare basic chatbot Q&A\n",
    "qa_pairs = {\n",
    "    \"hello\": \"Hi there! How can I help you today?\",\n",
    "    \"how are you\": \"I'm a bot, but I'm functioning as expected!\",\n",
    "    \"what is your name\": \"I'm a basic NLP chatbot using NLTK.\",\n",
    "    \"bye\": \"Goodbye! Take care!\",\n",
    "    \"what is nlp\": \"NLP stands for Natural Language Processing.\"\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 4: Clean and preprocess input\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Step 5: Generate response\n",
    "def generate_response(user_input):\n",
    "    processed_input = preprocess(user_input)\n",
    "    for question in qa_pairs:\n",
    "        if question in processed_input:\n",
    "            return qa_pairs[question]\n",
    "    return \"I'm not sure I understand. Try asking something else.\"\n",
    "\n",
    "# Step 6: Chat loop\n",
    "print(\"ChatBot ðŸ¤– is ready! Type 'bye' to exit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if 'bye' in user_input.lower():\n",
    "        print(\"ChatBot: Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "    print(\"ChatBot:\", generate_response(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0c856-4de6-4c62-95bf-8eb6026fcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow keras pickle nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c4330-7d09-41d9-9e0c-ed1f5e38d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle   # Correct way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641a5f9-0499-4344-9506-a58e0693c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import random\n",
    "\n",
    "# Load data from intents.json\n",
    "\n",
    "data_file = open('intents.json').read()\n",
    "\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e6dfb-135a-48fc-8bcb-24317ac20c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a96c1-79d9-429d-ab87-7c13afe11782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26223f-eb42-4f79-a96c-d82630cd4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a95ba1-3f80-4d6d-bf66-a945d5f6263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A bird in the hand is worth two in the bush\",\n",
    "    \"Actions speak louder than words\"\n",
    "]\n",
    "\n",
    "# Tokenize, lemmatize, and remove stopwords\n",
    "tokenized_data = []\n",
    "for sentence in data:\n",
    "    tokens = nltk.word_tokenize(sentence.lower())  # Tokenize and convert to lowercase\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]  # Remove stop words\n",
    "    tokenized_data.append(filtered_tokens)\n",
    "\n",
    "# Remove duplicate words\n",
    "for i in range(len(tokenized_data)):\n",
    "    tokenized_data[i] = list(set(tokenized_data[i]))\n",
    "\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74804b-fd77-4924-b055-1b27f8cbc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "\n",
    "training = []\n",
    "\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "\n",
    "    bag = []\n",
    "\n",
    "    pattern_words = doc[0]\n",
    "\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "\n",
    "    \n",
    "\n",
    "    for w in words:\n",
    "\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    \n",
    "\n",
    "    output_row = list(output_empty)\n",
    "\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# Shuffle and convert to numpy array\n",
    "\n",
    "random.shuffle(training)\n",
    "\n",
    "training = np.array(training)\n",
    "\n",
    "# Separate features and labels\n",
    "\n",
    "train_x = list(training[:,0])\n",
    "\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413113ac-9f3a-4d00-bb56-70bafb2a9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "\n",
    "model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "# Save the model\n",
    "\n",
    "model.save('chatbot_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f83be-af40-4dbf-8d47-f15417602dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hii\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I don't understand, but it sounds interesting!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hey there!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: See you later!\n"
     ]
    }
   ],
   "source": [
    "greetings = [\"hi\", \"hello\", \"hey\"]\n",
    "questions = [\"how are you\", \"what's up?\"]\n",
    "responses = [\"I'm doing well, thanks for asking!\", \"Just hanging out, waiting to chat!\"]\n",
    "farewells = [\"bye\", \"goodbye\", \"see you later\"]\n",
    "\n",
    "\n",
    "def respond(message):\n",
    "  message = message.lower()\n",
    "  if message in greetings:\n",
    "    return \"Hey there!\"\n",
    "  elif message in questions:\n",
    "    return responses[0]  # Choose a random response from responses list\n",
    "  elif message in farewells:\n",
    "    return \"See you later!\"\n",
    "  else:\n",
    "    return \"I don't understand, but it sounds interesting!\"\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "  user_input = input(\"You: \")\n",
    "  if user_input.lower() == \"quit\":\n",
    "    break\n",
    "  print(\"Chatbot:\", respond(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868fa99e-2e0e-4ba7-960d-fb88903a2c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bab51-d1e2-48d7-a544-ff787e75f95e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
